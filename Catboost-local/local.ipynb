{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "import matplotlib.pyplot as plt\n",
    "from src.features import preprocess,raw\n",
    "from src.utils.config import TRAIN_PATH_CLICK,TRAIN_PATH_SMS,TEST_PATH\n",
    "from src.train import pipeline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datap = 's3://data-lake-v2/cdp_clients_data/fsi/processed_data/stanbic_credit_scoring_updated/part-00000-28facaeb-3c6b-4b4b-a539-b569a6c978a8-c000.csv'\n",
    "data = pd.read_csv(datap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.preprocess(train=False,validation_path='./data/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import PROCESSED_TEST_PATH\n",
    "data = pd.read_pickle(PROCESSED_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING DATA FROM S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = [ pd.read_csv('s3://'+file) for file in fs.ls('s3://datateam-ml/Adrenaline-November-CTR/data/train') ]\n",
    "data = pd.concat(raw_data)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "#preprocess the input file and save the transformed data as a pickle file\n",
    "# preprocess.preprocess(dataframe=data,train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess.handle_nan(data,fillna='missing',drop_outliers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reindex(columns = input_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def load_attributes(data):\n",
    "    num_attributes = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_attributes = data.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    try:\n",
    "        num_attributes.remove(\"event_type\")\n",
    "        num_attributes.remove('customer_class')\n",
    "        cat_attributes.append('customer_class')\n",
    "        data['customer_class'] = self.data['customer_class'].astype(str)\n",
    "    except:\n",
    "        pass\n",
    "    return num_attributes, cat_attributes\n",
    "def identify_columns(data, high_dim=100, verbose=True, save_output=True):\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "        This funtion takes in the data, identify the numerical and categorical\n",
    "        attributes and stores them in a list\n",
    "        \n",
    "    \"\"\"\n",
    "    num_attributes, cat_attributes = load_attributes(data)\n",
    "        \n",
    "    low_cat = []\n",
    "    hash_features = []\n",
    "    dict_file = {}\n",
    "    input_columns = [cols for cols in data.columns]\n",
    "    input_columns.remove('event_type')\n",
    "    input_columns.remove('msisdn.1')\n",
    "    for item in cat_attributes:\n",
    "        if data[item].nunique() > high_dim:\n",
    "            if verbose:\n",
    "                print('\\n {} has a high cardinality. It has {} unique attributes'.format(item, data[item].nunique()))\n",
    "            hash_features.append(item)\n",
    "        else:\n",
    "            low_cat.append(item)\n",
    "    if save_output:\n",
    "        dict_file['num_feat'] = num_attributes\n",
    "        dict_file['cat_feat'] = cat_attributes\n",
    "        dict_file['hash_feat'] = hash_features\n",
    "        dict_file['lower_cat'] = low_cat\n",
    "        dict_file['input_columns'] = input_columns\n",
    "        store_attribute(dict_file)\n",
    "        print('\\nDone!. Data columns successfully identified and attributes are stored in /data/')\n",
    "def store_attribute(dict_file):\n",
    "    with open(r'./data/store_file.yaml', 'w') as file:\n",
    "        documents = yaml.dump(dict_file, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import input_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_columns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Features       Count of missing value\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "dataframe = dataframe.rename(columns={\"bad_behaviour\": \"event_type\"})\n",
    "dataframe = dataframe[~dataframe['customer_class'].isnull()]\n",
    "# dataframe = dataframe.dropna(thresh=data.shape[1]*0.6)\n",
    "dataframe.loc[:,'event_type'] = dataframe['event_type'].map({False:1, True:0})\n",
    "dataframe = preprocess.drop_cols(dataframe,columns=['msisdn','max_disbursement', 'max_extensions', 'max_dpd', 'telco', 'profile_identity', 'updated', \n",
    "                                                    'religion','occupation','social_media_presence', 'is_mother', 'phone_on_status', \n",
    "                                                    'roaming_status', 'sim_reg_status', 'sim_dnd_status', 'inbound_daily_count', \n",
    "                                                    'inbound_monthly_count', 'outbound_daily_count', 'outbound_monthly_count',\n",
    "                                                    'interactions_sms', 'roam_revenue', 'investment_score', 'ctr_score','interactions_click',\n",
    "                                                    'interaction_conversion'])\n",
    "dataframe = preprocess.handle_nan(dataframe,fillna='missing',drop_outliers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "\n",
    "sagemaker.Session().upload_data(bucket='datateam-ml', \n",
    "                              path='/home/ec2-user/SageMaker/Accessbank_CTR/Catboost-local/data/train', \n",
    "                              key_prefix='Adrenaline-November-CTR/data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "      'model': {\n",
    "          'iterations':500, \n",
    "          \"depth\" :6, \n",
    "          \"learning_rate\":0.3, \n",
    "          \"l2_leaf_reg\": 10, \n",
    "          \"loss_function\":'Logloss',\n",
    "          \"eval_metric\":'AUC'\n",
    "      },\n",
    "\n",
    "      'fit': {\n",
    "        'early_stopping_rounds': 10,\n",
    "        'verbose': 10\n",
    "      },\n",
    "\n",
    "      'fold': {\n",
    "        'n_splits': 5,\n",
    "        'shuffle': True,\n",
    "        'random_state': 0\n",
    "      }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_params = params['fold']\n",
    "model_params = params['model']\n",
    "fit_params = params['fit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(**fold_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = dataframe['event_type'].astype(int)\n",
    "X = dataframe.drop(['event_type'], axis=1).sample(n=9000, replace=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_indices = dataframe[dataframe.event_type == 1].index\n",
    "# sample_size = len(dataframe[dataframe.event_type == 0])\n",
    "random_indices = np.random.choice(positive_indices, 7500, replace=False)\n",
    "good = dataframe.loc[random_indices]\n",
    "bad = dataframe[dataframe['event_type']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad.reset_index(drop=True, inplace=True)\n",
    "good.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.concat([good,bad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = new_data['event_type'].astype(int)\n",
    "X = new_data.drop(['event_type'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL BUILDING FOR DIFFERENT ALG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " location_lga has a high cardinality. It has 658 unique attributes\n",
      "\n",
      " device_manufacturer has a high cardinality. It has 338 unique attributes\n",
      "\n",
      " device_model has a high cardinality. It has 2638 unique attributes\n",
      "\n",
      "Done!. Data columns successfully identified and attributes are stored in /data/\n",
      "['customer_value', 'gender', 'vas_subscriber', 'location_region', 'location_state', 'device_type', 'os_vendor', 'os_name', 'os_version', 'customer_class']\n",
      "(23378, 665)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid, lgb_pipeline = pipeline.fit_transform(dataframe,hash_size=200,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42,n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dd31dada4075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_valid = X_test\n",
    "y_valid = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[  25 1240]\n",
      " [  20 4560]]\n",
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.02      0.04      1265\n",
      "           1       0.79      1.00      0.88      4580\n",
      "\n",
      "    accuracy                           0.78      5845\n",
      "   macro avg       0.67      0.51      0.46      5845\n",
      "weighted avg       0.74      0.78      0.70      5845\n",
      "\n",
      "Accuracy : 0.784431\n",
      "f1 score : 0.878613\n"
     ]
    }
   ],
   "source": [
    "train_predictions = rfc.predict(X_valid)\n",
    "# print(rfc)\n",
    "# print(\"model score: %.3f\" % classifier.score(X_test, y_test))\n",
    "print('confusion matrix')\n",
    "print(metrics.confusion_matrix(y_valid.astype(int), train_predictions.astype(int)))\n",
    "print('classification report')\n",
    "print(metrics.classification_report(y_valid.astype(int), train_predictions.astype(int)))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y_valid.astype(int), train_predictions.astype(int))))\n",
    "print('f1 score : %f' % (metrics.fbeta_score(y_valid.astype(int), train_predictions.astype(int), beta=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import store_model,load_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_model('./model/lgb-model.pkl','./model/lgb-pipeline.pkl',(lgb,lgb_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CATBOOST MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cate_features_index = np.where(X.dtypes != float)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier, cv\n",
    "from utils.utils import load_pickle, print_devider\n",
    "from train.train import get_scores, log_plot\n",
    "from utils import plot_funcs as pf\n",
    "\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             confusion_matrix,\n",
    "                             roc_curve,\n",
    "                             roc_auc_score,\n",
    "                             precision_recall_curve,\n",
    "                             average_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y_true, y_pred):\n",
    "    return {\n",
    "      'accuracy': accuracy_score(y_true, y_pred),\n",
    "      'precision': precision_score(y_true, y_pred),\n",
    "      'recall': recall_score(y_true, y_pred),\n",
    "      'f1': f1_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "def devide_by_sum(x):\n",
    "    return x / x.sum()\n",
    "\n",
    "def log_plot(args, plot_func, fp):\n",
    "    if not isinstance(args, (tuple)):\n",
    "        args = (args,)\n",
    "\n",
    "    plot_func(*args, fp)\n",
    "    mlflow.log_artifact(fp)\n",
    "    os.remove(fp)\n",
    "    print(f'Logged {fp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = np.zeros(X.shape[1])\n",
    "for fold_no, (idx_train, idx_valid) in enumerate(skf.split(X, y)):\n",
    "    print_devider(f'Fold: {fold_no}')\n",
    "\n",
    "    X_train, X_valid = X.iloc[idx_train, :], X.iloc[idx_valid, :]\n",
    "    y_train, y_valid = y.iloc[idx_train], y.iloc[idx_valid]\n",
    "    model = CatBoostClassifier(**model_params)\n",
    "    model.fit(X_train, y_train,cat_features=cate_features_index,**fit_params,eval_set=(X_valid, y_valid))\n",
    "    feature_importances += devide_by_sum(model.feature_importances_) / skf.n_splits\n",
    "    train_predictions = model.predict(X_valid)\n",
    "    # evaluate\n",
    "    scores_valid = get_scores(y_valid, train_predictions)\n",
    "    print()\n",
    "    # print(\"model score: %.3f\" % classifier.score(X_test, y_test))\n",
    "    print('confusion matrix')\n",
    "    print(metrics.confusion_matrix(y_valid.astype(int), train_predictions.astype(int)))\n",
    "    print('classification report')\n",
    "    print(metrics.classification_report(y_valid.astype(int), train_predictions.astype(int)))\n",
    "    print('Accuracy : %f' % (metrics.accuracy_score(y_valid.astype(int), train_predictions.astype(int))))\n",
    "    print('f1 score : %f' % (metrics.fbeta_score(y_valid.astype(int), train_predictions.astype(int), beta=1)))\n",
    "    \n",
    "    # After you train the model using fit(), save like this - \n",
    "    model.save_model('model_name')    # extension not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(model.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4536+15+32+1224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape[0]/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "# print(\"model score: %.3f\" % classifier.score(X_test, y_test))\n",
    "print('confusion matrix')\n",
    "print(metrics.confusion_matrix(y_test.astype(int), train_predictions.astype(int)))\n",
    "print('classification report')\n",
    "print(metrics.classification_report(y_test.astype(int), train_predictions.astype(int)))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y_test.astype(int), train_predictions.astype(int))))\n",
    "print('f1 score : %f' % (metrics.fbeta_score(y_test.astype(int), train_predictions.astype(int), beta=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_test = preprocess.clean_data(keep_data)\n",
    "# preprocess.map_target(keep_data,'event_type')\n",
    "y = keep_data['event_type']\n",
    "# keep_data = preprocess.handle_nan(keep_data,fillna='missing',drop_outliers=True)\n",
    "preprocess.drop_cols(keep_data,['event_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then, later load - \n",
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier()      # parameters not required.\n",
    "model.load_model('mlruns/1/fa5d90cfe4574d64b7ebf5325b09810f/artifacts/model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = full_pipeline.transform(keep_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rf.predict(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [rfc,model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = load_pickle('./model/pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = load_pickle('./model/rfc-model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model,rf,lgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf mlruns/2/68681dbccaea43268478bdd2b1e6b12f/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow,os\n",
    "from collections import defaultdict\n",
    "mlflow.set_experiment('Access bank CTR exp')\n",
    "\n",
    "with mlflow.start_run(run_name='Access-bank-model-exp') as run:\n",
    "    data = raw.read_data('./data/validation.csv')\n",
    "    preprocess.map_target(data,'event_type')\n",
    "    y_valid = data['event_type']\n",
    "    data = preprocess.handle_nan(data,fillna='missing',drop_outliers=True)\n",
    "    \n",
    "    roc_list = {}\n",
    "    for model in model_list:\n",
    "        print(f'\\n{model.__class__.__name__}')\n",
    "        if model.__class__.__name__== 'CatBoostClassifier':\n",
    "            preprocess.drop_cols(data,['msisdn.1','event_type'])\n",
    "            new_data = data\n",
    "        elif model.__class__.__name__== 'RandomForestClassifier':\n",
    "            preprocess.drop_cols(data,['msisdn'])#'msisdn.1','event_type',\n",
    "            new_data = full_pipeline.transform(data)\n",
    "        else:\n",
    "            new_data = lgb_pipeline.transform(data)\n",
    "        \n",
    "        # predict\n",
    "        y_valid_proba = model.predict_proba(new_data)[:, 1]\n",
    "        y_valid_pred = model.predict(new_data)\n",
    "\n",
    "        scores = defaultdict(int)\n",
    "        # evaluate\n",
    "        scores_valid = get_scores(y_valid, y_valid_pred)\n",
    "\n",
    "        # record scores\n",
    "        for k, v in scores_valid.items():\n",
    "            scores[k] += v \n",
    "\n",
    "        # scores\n",
    "        log_plot(scores, pf.scores, f'{model.__class__.__name__}-scores.png')\n",
    "\n",
    "        # confusion matrix\n",
    "        cm = metrics.confusion_matrix(y_valid, y_valid_pred)\n",
    "        log_plot(cm, pf.confusion_matrix, f'{model.__class__.__name__}-confusion_matrix.png')\n",
    "        \n",
    "        # roc curve\n",
    "        fpr, tpr, _ = roc_curve(y_valid, y_valid_proba)\n",
    "        roc_auc = roc_auc_score(y_valid, y_valid_pred)\n",
    "        roc_list[f'{model.__class__.__name__}'] = [fpr,tpr,roc_auc]\n",
    "        \n",
    "    log_plot((model_list,roc_list), pf.multiple_roc_curve, 'roc_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw.read_data(path='./data/validation.csv')\n",
    "preprocess.map_target(data,'event_type')\n",
    "data = preprocess.handle_nan(data,fillna='missing',drop_outliers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_test = preprocess.clean_data(keep_data)\n",
    "# preprocess.map_target(keep_data,'event_type')\n",
    "y = data['event_type']\n",
    "# keep_data = preprocess.handle_nan(keep_data,fillna='missing',drop_outliers=True)\n",
    "test = preprocess.drop_cols(data,['msisdn.1','event_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)\n",
    "# print(\"model score: %.3f\" % classifier.score(X_test, y_test))\n",
    "print('confusion matrix')\n",
    "print(metrics.confusion_matrix(y.astype(int), results.astype(int)))\n",
    "print('classification report')\n",
    "print(metrics.classification_report(y.astype(int), results.astype(int)))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y.astype(int), results.astype(int))))\n",
    "print('f1 score : %f' % (metrics.fbeta_score(y.astype(int), results.astype(int), beta=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/ec2-user/SageMaker/Accessbank_CTR/Catboost-local/foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "shutil.copy('/home/ec2-user/SageMaker/Accessbank_CTR/Catboost-local/test.csv', '/home/ec2-user/SageMaker/Accessbank_CTR/Catboost-sagemaker/container/local_test/test_dir/input/data/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=500, depth=6, learning_rate=0.3, l2_leaf_reg = 10, loss_function='Logloss',eval_metric='AUC')\n",
    "model.fit(X_train, y_train,cat_features=cate_features_index,eval_set=(X_test, y_test),early_stopping_rounds=10,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(y_valid_pred,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'msisdn':y_valid.index,'results':y_valid_pred.flatten()}).to_csv('hey.csv',index=False,\n",
    "                                                                       sep='|',header=['msisdn','ctr_access_bank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.train import get_scores\n",
    "import utils.plot_funcs as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_valid = get_scores(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devide_by_sum(x):\n",
    "    return x / x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devide_by_sum(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis = defaultdict(int)\n",
    "for i in range(0,5):\n",
    "    for k, v in scores_valid.items():\n",
    "        mis[k]+= v/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.scores(mis,'scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(model.feature_names_)\n",
    "pf.feature_importance(features, feature_importances, 'Feature Importance','feature_importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "metrics.append({\n",
    "              'name': ['AUC'],\n",
    "              'values': model.evals_result_['learn']['Precision'],\n",
    "              'best_iteration': model.best_iteration_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.metric(metrics, 'metric_history.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_valid, y_valid_pred)\n",
    "pf.confusion_matrix(cm, 'confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evals_result_['learn'][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
