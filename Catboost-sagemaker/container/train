#!/usr/bin/env python

from __future__ import print_function

import os,json,sys,subprocess,traceback

import argparse
import os,sys
from catboostModel import train

# These are the paths to where SageMaker mounts interesting things in your container.
prefix = '/opt/ml/'
input_path = os.path.join(prefix,'input/data')
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name = 'training'
training_path = os.path.join(input_path, channel_name)

# Dataset used for training the model
# input_file = os.path.join(training_path, os.listdir(training_path)[0])

# Execute your training algorithm.



if __name__ == '__main__':
    print('{}. {} doesnt exist'.format(os.path.exists(param_path),param_path))
    args_parser = argparse.ArgumentParser()
    args_parser.add_argument(
        '--data-dir',
        default='/opt/ml/input/data/training',
        type=str,
        help='The directory where the input data is stored. Default: /opt/ml/input/data/training. This '
             'directory corresponds to the SageMaker channel named \'training\', which was specified when creating '
             'our training job on SageMaker')

    args_parser.add_argument(
        '--model-dir',
        default='/opt/ml/model',
        type=str,
        help='The directory where the model will be stored. Default: /opt/ml/model. This directory should contain all '
             'final model artifacts as Amazon SageMaker copies all data within this directory as a single object in '
             'compressed tar format.')
             
    #args_parser.add_argument('--epochs', type=int, default=os.environ['EPOCHS'])
    #args_parser.add_argument('--learning_rate', type=float, default=os.environ['LEARNING_RATE'])
    #args_parser.add_argument('--batch_size', type=int, default=os.environ['BATCH_SIZE'])
    #args_parser.add_argument('--gpu_count', type=int, default=os.environ['GPU_COUNT'])
    

    args, _ = args_parser.parse_known_args()
    model_dir     = args.model_dir
    #epochs         = args.epochs
    #learning_rate  = args.learning_rate
    #batch_size     = args.batch_size
    #gpu_count      = args.gpu_count
    
    train(model_dir,output_path, training_path, channel_name)
    print('Training complete.')
    
    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)